#!/usr/bin/env python3
"""
TDAC-seq processing pipeline: Preprocessing, alignment, deduplication,
consensus variant calling with majority voting, and plotting.
WITH PRIMER TRIMMING + CLUSTERING-BASED DEAMINATION MASKING.
"""
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import pickle
from tdac_seq.ddda_dataset import ddda_dataset
from tdac_seq.utils import export_to_tsv
from matplotlib.colors import ListedColormap
import os
import sys 
import logging
import subprocess 
import argparse
import mappy
import pysam
from collections import defaultdict
import re
import multiprocessing
from functools import partial
import json




# ==================== Configuration ====================




SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
base_dir = os.path.dirname(SCRIPT_DIR)




RUN_TDAC_SH = os.path.join(SCRIPT_DIR, "basecall_demultiplex.sh")
RUN_PREFIX = "DS98"
PRIMER_SEQ = "CACTGACGT"  # Your barcode/adapter sequence




genome_file = "/home/dan/Index/GRCh38.primary_assembly.genome.fa" 
bias_model_path = os.path.join(base_dir, "data/bias_dict.pkl")




ref_range_dict = {
    'HCT_distal': ('chr8', 129706076, 129712513)
}
locus = "HCT_distal" 




figure_dir = os.path.join(base_dir, "figures")
data_dir = os.path.join(base_dir, "data") 
os.makedirs(figure_dir, exist_ok=True)
os.makedirs(data_dir, exist_ok=True)


# Load bias model
try:
    with open(bias_model_path, "rb") as f:
        bias_model = pickle.load(f)
except FileNotFoundError:
    print(f"ERROR: Bias model not found at {bias_model_path}")
    sys.exit(1)


# Custom colormaps
base_colors_blue = plt.cm.Blues(np.linspace(0, 1, 256))
base_colors_blue[0] = [0.97, 0.97, 0.97, 1]
base_colors_blue[-1] = [0, 0, 0.5, 1]
custom_cmap_blue = ListedColormap(base_colors_blue)


base_colors_red = plt.cm.Reds(np.linspace(0, 1, 256))
base_colors_red[0] = [0.97, 0.97, 0.97, 1]
base_colors_red[-1] = [0.5, 0, 0, 1]
custom_cmap_red = ListedColormap(base_colors_red)


base_colors_purple = plt.cm.Purples(np.linspace(0, 1, 256))
base_colors_purple[0] = [0.97, 0.97, 0.97, 1]
base_colors_purple[-1] = [0.25, 0, 0.5, 1]
custom_cmap_purple = ListedColormap(base_colors_purple)


# ==============================================================================
# === PREPROCESSING FUNCTIONS ==================================================
# ==============================================================================


def run_preprocessing_shell(base_dir, run_tdac_sh, genome_file, start_type, input_path):
    """Executes basecall_demultiplex.sh for preprocessing."""
    print(f"\n{'='*60}")
    print(f"ðŸš€ Launching TDAC Preprocessing: {os.path.basename(run_tdac_sh)}")
    print(f"  Project Root: {base_dir}")
    print(f"  Genome Path: {genome_file}") 
    print(f"  Start Type: {start_type}")
    print(f"  Input Path: {input_path}")
    print(f"{'='*60}")
    
    try:
        command = [
            "mamba", "run", "-n", "nano", "bash", 
            run_tdac_sh, base_dir, genome_file, start_type, input_path
        ]
        result = subprocess.run(command, check=True, capture_output=False, text=True)
        print(f"\n Preprocessing complete. Exit code: {result.returncode}")
    except subprocess.CalledProcessError as e:
        print(f"\n ERROR: Shell script failed with exit code {e.returncode}")
        print(f"Stderr:\n{e.stderr}")
        sys.exit(1)
    except FileNotFoundError:
        print(f"\n ERROR: basecall_demultiplex.sh not found at {run_tdac_sh}")
        sys.exit(1)




def load_sample_config(base_dir):
    """
    Load unique samples and point to their MERGED FASTQ files.
    """
    rename_table_path = os.path.join(base_dir, "data", "rename_table.tsv")
    if not os.path.exists(rename_table_path):
        print(f"ERROR: Rename table not found at {rename_table_path}")
        print("This should be generated by basecall_demultiplex.sh")
        sys.exit(1)
        
    df_rename = pd.read_csv(rename_table_path, sep='\t')
    
    sample_dict = {}
    RUN_PREFIX = "DS98" 
    PER_SAMPLE_DIR = os.path.join(base_dir, "data", "per_sample")
    
    for sample_name in df_rename['Sample'].unique():
        merged_fastq = os.path.join(
            PER_SAMPLE_DIR,
            f"{RUN_PREFIX}_{sample_name}.fastq.gz"
        )
        
        if os.path.exists(merged_fastq):
            sample_dict[sample_name] = merged_fastq
        else:
            print(f"WARNING: Merged FASTQ not found for {sample_name}: {merged_fastq}")
    
    print(f"\n[CONFIG] Loaded {len(sample_dict)} unique samples:")
    for name, path in sample_dict.items():
        print(f"  âœ“ {name}: {os.path.basename(path)}")
    
    if not sample_dict:
        print("\n[ERROR] No valid samples found!")
        sys.exit(1)
    
    return sample_dict




# ==============================================================================
# === CONSENSUS VALIDATION FUNCTIONS ==========================================
# ==============================================================================




def validate_consensus_against_ref(consensus_seq, ref_seq, aligner, cluster_id, logger=None):
    """
    Align consensus to reference and compute error metrics.
    """
    if not consensus_seq or not ref_seq:
        return {
            'identity': 0, 
            'n_mismatches': 0, 
            'n_insertions': 0,
            'n_deletions': 0,
            'alignment_length': 0,
            'status': 'empty_sequence'
        }
    
    try:
        hits = list(aligner.map(consensus_seq, cs=True))
        
        if not hits:
            if logger:
                logger.warning(f"Cluster {cluster_id}: consensus did not align to reference")
            return {
                'identity': 0, 
                'n_mismatches': 0, 
                'n_insertions': 0,
                'n_deletions': 0,
                'alignment_length': 0,
                'status': 'no_hit'
            }
        
        hit = hits[0]
        
        tokens = re.findall(r'(:[0-9]+|\*[a-z][a-z]|\+[a-z]+|\-[a-z]+)', hit.cs)
        
        mismatches = 0
        insertions = 0
        deletions = 0
        matches = 0
        
        for token in tokens:
            op = token[0]
            val = token[1:]
            
            if op == ':':
                matches += int(val)
            elif op == '*':
                mismatches += 1
            elif op == '+':
                insertions += len(val)
            elif op == '-':
                deletions += len(val)
        
        consensus_len = len(consensus_seq)
        identity_pct = (matches / consensus_len * 100) if consensus_len > 0 else 0
        total_aligned = matches + mismatches + insertions
        
        qc_dict = {
            'identity': round(identity_pct, 2),
            'n_mismatches': mismatches,
            'n_insertions': insertions,
            'n_deletions': deletions,
            'matches': matches,
            'alignment_length': total_aligned,
            'consensus_length': consensus_len,
            'ref_start': hit.r_st,
            'ref_end': hit.r_en,
            'query_start': hit.q_st,
            'query_end': hit.q_en,
            'mapping_quality': hit.mapq,
            'status': 'aligned'
        }
        
        if identity_pct < 90:
            if logger:
                logger.warning(f"Cluster {cluster_id}: LOW consensus identity={identity_pct:.1f}%, "
                               f"mismatches={mismatches}, indels={insertions+deletions}")
        
        return qc_dict
        
    except Exception as e:
        if logger:
            logger.error(f"Cluster {cluster_id}: Error validating consensus: {e}")
        return {
            'identity': 0,
            'n_mismatches': 0,
            'n_insertions': 0,
            'n_deletions': 0,
            'alignment_length': 0,
            'status': 'error',
            'error_msg': str(e)
        }




# ==============================================================================
# === CONSENSUS SCRIPT FUNCTIONS ===============================================
# ==============================================================================




def get_variants_from_local_ref(aligner, consensus_seq, cluster_id, offset):
    """Aligns consensus sequence and returns all variants."""
    try:
        hits = list(aligner.map(consensus_seq, cs=True))
    except Exception:
        return []
    
    if not hits: 
        return []
    
    hit = hits[0]
    curr_ref_relative = hit.r_st
    variants = []
    
    tokens = re.findall(r'(:[0-9]+|\*[a-z][a-z]|\+[a-z]+|\-[a-z]+)', hit.cs)
    
    for token in tokens:
        op, val = token[0], token[1:]
        
        if op == ':':
            curr_ref_relative += int(val)
        elif op == '*':
            ref_base, alt_base = val[0].upper(), val[1].upper()
            genomic_pos = curr_ref_relative + offset
            variants.append({
                'Cluster_ID': cluster_id,
                'Genomic_Pos': genomic_pos,
                'Ref': ref_base,
                'Alt': alt_base,
                'Type': 'SNP'
            })
            curr_ref_relative += 1
        elif op == '-':
            curr_ref_relative += len(val)
        elif op == '+':
            genomic_pos = curr_ref_relative + offset
            variants.append({
                'Cluster_ID': cluster_id,
                'Genomic_Pos': genomic_pos,
                'Ref': '-',
                'Alt': val.upper(),
                'Type': 'INSERTION'
            })
            
    return variants




def trim_primer_sequence(consensus_seq, primer_seq, logger=None):
    """
    Remove primer sequence from start of consensus.
    
    Args:
        consensus_seq: Full consensus sequence from majority voting
        primer_seq: Known primer sequence to remove (e.g., "CACTGACGT")
        logger: logging object
    
    Returns:
        tuple: (trimmed_consensus, n_bases_trimmed)
    """
    # Check if consensus starts with primer
    if consensus_seq.startswith(primer_seq):
        trimmed = consensus_seq[len(primer_seq):]
        if logger:
            logger.info(f"Trimmed {len(primer_seq)}bp primer from consensus start")
        return trimmed, len(primer_seq)
    
    # Check if consensus starts with primer allowing for 1-2 mismatches (Nanopore errors)
    primer_len = len(primer_seq)
    start_seq = consensus_seq[:primer_len]  
    
    # Count mismatches
    mismatches = 0
    for i in range(min(len(primer_seq), len(start_seq))):
        if start_seq[i] != primer_seq[i]:
            mismatches += 1
    
    if mismatches <= 2:  # Allow up to 2 mismatches
        trimmed = consensus_seq[primer_len:]
        if logger:
            logger.info(f"Trimmed {primer_len}bp primer from consensus start ({mismatches} mismatches)")
        return trimmed, primer_len
    
    # If not found, return original
    if logger:
        logger.warning(f"Primer sequence not found at consensus start. First 20bp: {consensus_seq[:20]}")
    return consensus_seq, 0




def process_cluster(cluster_tuple, ref_fa_path, ref_seq, chrom, start_pos, offset, 
                   logger=None, primer_seq="CACTGACGT", edited_positions=None):
    """
    Generate consensus via majority voting against reference.
    Handles primer trimming from reads before alignment.
    MASKS positions that are part of cluster's deamination pattern.
    
    Args:
        edited_positions: Set of position indices to mask (from cluster's edit pattern)
    """
    import mappy
    
    cluster_id, read_data = cluster_tuple
    
    try:
        read_names, sequences = zip(*read_data)
        read_names = list(read_names)
        sequences = list(sequences)
    except ValueError:
        return {
            'variants': [], 
            'msa_string': '', 
            'cluster_id': cluster_id, 
            'cluster_size': 0,
            'consensus_qc': {'status': 'empty_cluster'}
        }
    
    cluster_size = len(sequences)
    
    # Initialize aligner
    try:
        aligner = mappy.Aligner(ref_fa_path, preset="map-ont")
        if not aligner:
            return {
                'variants': [], 
                'msa_string': '', 
                'cluster_id': cluster_id, 
                'cluster_size': 0,
                'consensus_qc': {'status': 'aligner_init_failed'}
            }
    except Exception as e:
        return {
            'variants': [], 
            'msa_string': '', 
            'cluster_id': cluster_id, 
            'cluster_size': 0,
            'consensus_qc': {'status': 'aligner_error'}
        }
    
    # ALIGN each read to reference independently
    pileup = {}  # position -> {base -> count}
    
    for read_name, sequence in zip(read_names, sequences):
        try:
            # IMPORTANT: Trim primer from read BEFORE alignment
            trimmed_read, n_trimmed = trim_primer_sequence(sequence, primer_seq, logger=None)
            
            hits = list(aligner.map(trimmed_read, cs=True))
            if not hits:
                if logger:
                    logger.warning(f"Cluster {cluster_id}: Read {read_name[:16]}... did not align")
                continue
            
            hit = hits[0]
            
            # Parse the alignment
            tokens = re.findall(r'(:[0-9]+|\*[a-z][a-z]|\+[a-z]+|\-[a-z]+)', hit.cs)
            
            curr_ref_pos = hit.r_st
            curr_query_pos = 0
            
            for token in tokens:
                op = token[0]
                val = token[1:]
                
                if op == ':':  # Matches
                    length = int(val)
                    for i in range(length):
                        ref_pos = curr_ref_pos + i
                        query_base = trimmed_read[curr_query_pos + i]
                        
                        if ref_pos not in pileup:
                            pileup[ref_pos] = {}
                        pileup[ref_pos][query_base] = pileup[ref_pos].get(query_base, 0) + 1
                    
                    curr_ref_pos += length
                    curr_query_pos += length
                
                elif op == '*':  # Mismatch
                    ref_base = val[0].upper()
                    alt_base = val[1].upper()
                    
                    if curr_ref_pos not in pileup:
                        pileup[curr_ref_pos] = {}
                    pileup[curr_ref_pos][alt_base] = pileup[curr_ref_pos].get(alt_base, 0) + 1
                    
                    curr_ref_pos += 1
                    curr_query_pos += 1
                
                elif op == '+':  # Insertion
                    curr_query_pos += len(val)
                
                elif op == '-':  # Deletion
                    curr_ref_pos += len(val)
        
        except Exception as e:
            if logger:
                logger.error(f"Cluster {cluster_id}: Error aligning {read_name}: {e}")
            continue
    
    # Safe default
    if edited_positions is None:
        edited_positions = set()
    
    # MAJORITY VOTE: call consensus at each position
    consensus_bases = []
    consensus_positions = sorted(pileup.keys())
    masked_positions = []
    
    for pos in consensus_positions:
        bases_at_pos = pileup[pos]
        
        if pos in edited_positions:
            # This position is part of cluster's deamination pattern - MASK IT
            masked_positions.append({
                'pos': pos,
                'ref': ref_seq[pos] if pos < len(ref_seq) else 'N',
                'pileup': dict(bases_at_pos),  # Convert to dict for JSON serialization
                'reason': 'cluster_edit_pattern'
            })
            # Use reference base as placeholder (flagged as masked)
            consensus_bases.append(ref_seq[pos] if pos < len(ref_seq) else 'N')
        else:
            # Normal position - call consensus
            majority_base = max(bases_at_pos.items(), key=lambda x: x[1])[0]
            consensus_bases.append(majority_base)
    
    consensus_seq = ''.join(consensus_bases)
    
    if len(consensus_seq) == 0:
        return {
            'variants': [], 
            'msa_string': '', 
            'cluster_id': cluster_id, 
            'cluster_size': cluster_size,
            'consensus_qc': {'status': 'no_consensus'}
        }
    
    # Validate consensus against reference
    consensus_qc = validate_consensus_against_ref(
        consensus_seq, ref_seq, aligner, cluster_id, logger=logger
    )
    
    # Get variants
    variants = get_variants_from_local_ref(aligner, consensus_seq, cluster_id, offset)
    
    for v in variants:
        v['Cluster_Size'] = cluster_size
        v['Consensus_Identity'] = consensus_qc.get('identity', np.nan)
        v['Consensus_Mismatches'] = consensus_qc.get('n_mismatches', 0)
        v['Consensus_Insertions'] = consensus_qc.get('n_insertions', 0)
        v['Consensus_Deletions'] = consensus_qc.get('n_deletions', 0)
    
    return {
        'variants': variants, 
        'msa_string': f">consensus_{cluster_id}\n{consensus_seq}\n>reference\n{ref_seq}", 
        'cluster_id': cluster_id,
        'cluster_size': cluster_size,
        'consensus': consensus_seq,
        'consensus_qc': consensus_qc,
        'primer_trimmed': True,
        'masked_positions': masked_positions,
        'n_masked': len(masked_positions)
    }




def process_cluster_wrapper(task_with_edits, ref_fa_path, ref_seq, chrom, 
                           start_pos, offset, logger, primer_seq):
    """
    Wrapper to unpack task tuple with edited positions set.
    """
    cluster_id, read_data, edited_positions = task_with_edits
    cluster_tuple = (cluster_id, read_data)
    
    return process_cluster(
        cluster_tuple, ref_fa_path, ref_seq, chrom, start_pos, offset,
        logger=logger, primer_seq=primer_seq, edited_positions=edited_positions
    )




def run_consensus_analysis(sample_name, locus, base_dir, sample_dir, fastq_path, 
                         ref_range_dict, target_start_offset, logger=None, n_cores=None,
                         primer_seq="CACTGACGT", ref_seqs=None, cluster_edited_positions=None):
    """Parallel consensus/variant calling with majority voting and clustering-based masking."""
    if logger is None:
        logger = logging.getLogger(sample_name)
        logger.handlers = []
        logger.setLevel(logging.INFO)
        log_path = os.path.join(sample_dir, f"{sample_name}_consensus.log")
        fh = logging.FileHandler(log_path, mode='w')
        fh.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))
        logger.addHandler(fh)
    
    logger.info(f"\n{'-'*50}")
    logger.info(f"Consensus Variant Analysis for {sample_name}")
    logger.info(f"Masking: Cluster-based deamination patterns")
    logger.info(f"{'-'*50}")
    
    grouping_path = f"{sample_dir}/{sample_name}_grouping.tsv"
    ref_fa_path = os.path.join(base_dir, "ref.fa") 
    output_csv = f"{sample_dir}/{sample_name}_cluster_consensus_TRUE_variants.csv"
    qc_output_csv = f"{sample_dir}/{sample_name}_consensus_qc_report.csv"
    msa_output_path = f"{sample_dir}/{sample_name}_cluster_msa.fasta"
    masked_output_json = f"{sample_dir}/{sample_name}_cluster_masked_positions.json"
    min_coverage = 4
    
    N_CORES = max(1, n_cores if n_cores else (multiprocessing.cpu_count() - 2))
    
    # Check required files exist
    if not all(os.path.exists(p) for p in [grouping_path, fastq_path]):
        logger.error("[ERROR] Missing required files (grouping.tsv or fastq)")
        return
    
    # GET REFERENCE SEQUENCE - use passed ref_seqs, don't load again!
    if ref_seqs is None:
        logger.error("[ERROR] ref_seqs not provided!")
        return
    
    if locus not in ref_seqs:
        logger.error(f"[ERROR] Locus '{locus}' not in ref_seqs. Available: {list(ref_seqs.keys())}")
        return
    
    ref_seq = ref_seqs[locus]
    logger.info(f"Using reference sequence: {len(ref_seq)} bp")
    
    # Extract reference region info (for offset calculations)
    chrom, start, end = ref_range_dict[locus]
    
    # Load grouping and build clusters
    df_groups = pd.read_csv(grouping_path, sep='\t', header=None, names=['Read_ID', 'Cluster_ID'])
    cluster_map = dict(zip(df_groups['Read_ID'], df_groups['Cluster_ID']))
    
    # Store (read_name, sequence) tuples
    clusters = defaultdict(list)
    logger.info(f"Indexing FASTQ...")
    with pysam.FastxFile(fastq_path) as fh:
        for entry in fh:
            if entry.name in cluster_map:
                c_id = cluster_map[entry.name]
                clusters[c_id].append((entry.name, entry.sequence))
    
    tasks = []
    for c_id, read_data in clusters.items():
        if len(read_data) >= min_coverage:
            # Get edited positions for this cluster (from clustering pattern)
            edited_pos = set()
            if cluster_edited_positions is not None and c_id in cluster_edited_positions:
                edited_pos = cluster_edited_positions[c_id]
            tasks.append((c_id, read_data, edited_pos))
    
    logger.info(f"Processing {len(tasks)} clusters (>={min_coverage} reads) on {N_CORES} cores...")
    
    # Create worker function
    worker_func = partial(
        process_cluster_wrapper,
        ref_fa_path=ref_fa_path, 
        ref_seq=ref_seq,
        chrom=chrom,
        start_pos=start,
        offset=target_start_offset,
        logger=logger,
        primer_seq=primer_seq
    )
    
    # Run consensus calling in parallel
    with multiprocessing.Pool(N_CORES) as pool:
        results = pool.map(worker_func, tasks) 
    
    # Collect results
    all_variants_raw = []
    all_qc_metrics = []
    all_masked_data = {}
    logger.info(f"Collecting results and writing consensuses to {msa_output_path}...")
    
    with open(msa_output_path, 'w') as msa_f:
        for res_dict in results:
            all_variants_raw.extend(res_dict['variants'])
            
            # Store QC metrics per cluster
            qc_row = {
                'Cluster_ID': res_dict['cluster_id'],
                'Cluster_Size': res_dict['cluster_size'],
                'Consensus_Status': res_dict['consensus_qc'].get('status', 'unknown'),
                'Identity_Pct': res_dict['consensus_qc'].get('identity', np.nan),
                'Mismatches': res_dict['consensus_qc'].get('n_mismatches', np.nan),
                'Insertions': res_dict['consensus_qc'].get('n_insertions', np.nan),
                'Deletions': res_dict['consensus_qc'].get('n_deletions', np.nan),
                'Num_Variants_Called': len(res_dict['variants']),
                'Num_Masked_Positions': res_dict.get('n_masked', 0)
            }
            all_qc_metrics.append(qc_row)
            
            # Store masked position data
            if res_dict.get('masked_positions'):
                all_masked_data[res_dict['cluster_id']] = res_dict['masked_positions']
            
            if res_dict['msa_string']:
                msa_f.write(f"# === CLUSTER {res_dict['cluster_id']} (Size={res_dict['cluster_size']}, "
                           f"Identity={res_dict['consensus_qc'].get('identity', 'N/A')}%, "
                           f"Masked={res_dict.get('n_masked', 0)}) ===\n")
                msa_f.write(res_dict['msa_string'])
                msa_f.write("\n\n")
    
    # Save QC report
    df_qc = pd.DataFrame(all_qc_metrics)
    df_qc.to_csv(qc_output_csv, index=False)
    logger.info(f"âœ“ Saved QC report: {qc_output_csv}")
    
    # Save masked positions data
    if all_masked_data:
        with open(masked_output_json, 'w') as f:
            json.dump(all_masked_data, f, indent=2, default=str)
        logger.info(f"âœ“ Saved masked positions: {masked_output_json}")
    
    # Flag low-quality consensuses
    low_quality = df_qc[df_qc['Identity_Pct'] < 90]
    logger.info(f"\nQC Summary: {len(low_quality)}/{len(df_qc)} clusters with identity <90%")
    
    if len(low_quality) > 0:
        logger.info("Low-quality clusters (Identity <90%):")
        for _, row in low_quality.iterrows():
            logger.info(f"  Cluster {int(row['Cluster_ID'])}: Identity={row['Identity_Pct']:.1f}%, "
                       f"Size={int(row['Cluster_Size'])}, Masked={int(row['Num_Masked_Positions'])}")
    
    # Masking summary
    total_masked = df_qc['Num_Masked_Positions'].sum()
    logger.info(f"\nMasking Summary:")
    logger.info(f"  Total positions masked: {int(total_masked)}")
    logger.info(f"  Clusters with masked positions: {len(all_masked_data)}")
    if len(df_qc) > 0:
        logger.info(f"  Avg masked per cluster: {total_masked/len(df_qc):.1f}")
    
    # Process variants
    if all_variants_raw:
        df_raw = pd.DataFrame(all_variants_raw)
        
        # Filter deamination artifacts
        is_C_to_T = (df_raw['Ref'] == 'C') & (df_raw['Alt'] == 'T')
        is_G_to_A = (df_raw['Ref'] == 'G') & (df_raw['Alt'] == 'A')
        df_filtered = df_raw[~(is_C_to_T | is_G_to_A)]
        
        df_filtered.to_csv(output_csv, index=False)
        n_deamination = len(df_raw) - len(df_filtered)
        logger.info(f"\nVariant Summary:")
        logger.info(f"  Total called: {len(df_raw)}")
        logger.info(f"  Deamination artifacts (Câ†’T/Gâ†’A): {n_deamination}")
        logger.info(f"  True variants: {len(df_filtered)}")
        
        if not df_filtered.empty:
            logger.info(f"\nTop 10 Variants:")
            summary = df_filtered.groupby(['Genomic_Pos', 'Ref', 'Alt']).size().rename('Count').sort_values(ascending=False).head(10)
            for (pos, ref, alt), count in summary.items():
                logger.info(f"  {pos}: {ref}â†’{alt} (n={count})")
    else:
        logger.info("No variants found")
    
    logger.info(f"\n{'-'*50}\nâœ… Finished Consensus Analysis\n{'-'*50}\n")




# ==============================================================================
# === (NOW SERIAL) WORKER FUNCTION =============================================
# ==============================================================================




def process_single_sample(sample_data, base_dir, ref_range_dict, ref_seqs, genome_file, 
                         bias_model, locus, figure_dir, data_dir, run_prefix,
                         n_cores_for_consensus, primer_seq="CACTGACGT"):
    """
    Function to process a single sample.
    This is now called SERIALLY from the main thread.
    """
    sample_name, fastq_file = sample_data
    sample_dir = os.path.join(data_dir, sample_name)
    os.makedirs(sample_dir, exist_ok=True)
    
    log_path = os.path.join(sample_dir, f"{sample_name}_processing.log")
    logger = logging.getLogger(sample_name)
    logger.handlers = []
    logger.setLevel(logging.INFO)
    fh = logging.FileHandler(log_path, mode='w')
    fh.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))
    logger.addHandler(fh)
    
    logger.info(f"\n{'='*60}\nProcessing: {sample_name}\n{'='*60}")
    
    try:
        # Initialize dataset
        ddda_data = ddda_dataset(
            ID=sample_name, 
            region_dict=ref_range_dict, 
            genome_file=genome_file, 
            fastq_file=fastq_file, 
            working_dir=base_dir
        )
        ddda_data.bias_model = bias_model
        
        # Align
        logger.info("Aligning reads...")
        ddda_data.align_reads(start_gap_threshold=500, end_gap_threshold=500)
        n_mapped = len(ddda_data.read_ids[locus])
        logger.info(f"MAPPED READS: {n_mapped}")

        # Export mapped reads
        logger.info("Exporting mapped reads...")
        mapped_read_ids = ddda_data.read_ids[locus]
        mapped_output = os.path.join(sample_dir, f"{sample_name}_mapped_reads.txt")
        with open(mapped_output, 'w') as f:
            for read_id in mapped_read_ids:
                f.write(f"{read_id}\n")
        logger.info(f"  âœ“ Exported {len(mapped_read_ids)} mapped read IDs to {mapped_output}")

        # If ddda_data has mapping positions, export those too
        if hasattr(ddda_data, 'read_positions') and locus in ddda_data.read_positions:
            positions = ddda_data.read_positions[locus]
            strands = ddda_data.read_strands[locus] if hasattr(ddda_data, 'read_strands') else None
            
            mapping_output = os.path.join(sample_dir, f"{sample_name}_read_mapping.tsv")
            with open(mapping_output, 'w') as f:
                f.write("Read_ID\tPosition\tStrand\n")
                for i, read_id in enumerate(mapped_read_ids):
                    strand = strands[i] if strands is not None else "."
                    f.write(f"{read_id}\t{positions[i]}\t{strand}\n")
            logger.info(f"  âœ“ Exported mapping positions to {mapping_output}")

        # Export TSVs
        logger.info("Exporting TSVs...")
        export_to_tsv(ddda_data, export_DddA_edit=True, export_del=False, 
                     export_ABE_edit=False, export_strand=True, 
                     export_dir=sample_dir, overwrite=False)
        
        # Deduplication
        logger.info("Deduplication...")
        grouping_path = f"{sample_dir}/{sample_name}_grouping.tsv"
        cluster_json_path = f"{sample_dir}/{sample_name}_clusters.json"
        ddda_data.dedup_all(
            grouping_export_path=grouping_path,
            cluster_data_export_json_path=cluster_json_path
        )
        
        # Fix grouping file
        logger.info("Converting indices to Read IDs...")
        if os.path.exists(grouping_path):
            df_grouping = pd.read_csv(grouping_path, sep='\t', header=None, names=['Read_Idx', 'Group_Idx'])
            actual_read_ids = np.array(ddda_data.read_ids[locus])
            if pd.api.types.is_integer_dtype(df_grouping['Read_Idx']):
                df_grouping['Read_ID'] = actual_read_ids[df_grouping['Read_Idx']]
                df_final = df_grouping[['Read_ID', 'Group_Idx']]
                df_final.to_csv(grouping_path, sep='\t', index=False, header=False)
                logger.info("  âœ“ Grouping file fixed")
        
        # Save deduplicated reads
        final_deduplicated_indices = ddda_data.deduped_inds[locus]
        final_list_path = f"{sample_dir}/{sample_name}_final_deduped_readIDs.txt"
        all_read_ids = np.array(ddda_data.read_ids[locus])
        final_read_names = all_read_ids[final_deduplicated_indices]
        np.savetxt(final_list_path, final_read_names, fmt='%s')
        
        n_deduped = len(final_deduplicated_indices)
        percent_kept = (n_deduped / n_mapped * 100) if n_mapped > 0 else 0
        logger.info(f"DEDUPLICATED: {n_deduped} ({percent_kept:.2f}% retained)")

        # Save pickle
        pkl_path = f"{sample_dir}/ddda_data_{sample_name}.pkl"
        with open(pkl_path, 'wb') as f:
            pickle.dump(ddda_data, f)
        
        # BUILD CLUSTER EDITED POSITIONS
        logger.info("Building cluster edit patterns from deamination data...")
        dedup_indices = ddda_data.deduped_inds[locus]
        all_edits = np.array(ddda_data.edit_dict[locus][dedup_indices].todense())
        
        # IMPORTANT: grouping_path has Read_IDs, not indices
        # We need to map: Read_ID -> full index -> dedup index
        df_groups = pd.read_csv(grouping_path, sep='\t', header=None, names=['Read_ID', 'Cluster_ID'])
        
        # Create mapping from full read index to dedup index
        full_to_dedup_map = {full_idx: dedup_idx for dedup_idx, full_idx in enumerate(dedup_indices)}
        
        cluster_edited_positions = {}
        all_read_ids = np.array(ddda_data.read_ids[locus])
        
        for cluster_id in df_groups['Cluster_ID'].unique():
            # Get the READ_IDs for this cluster
            cluster_read_ids = df_groups[df_groups['Cluster_ID'] == cluster_id]['Read_ID'].values
            
            # Map read IDs to full indices
            cluster_full_indices = np.where(np.isin(all_read_ids, cluster_read_ids))[0]
            
            # Convert to dedup indices (only those that were kept)
            cluster_dedup_indices = np.array([full_to_dedup_map[idx] for idx in cluster_full_indices if idx in full_to_dedup_map])
            
            if len(cluster_dedup_indices) > 0:
                # Use fancy indexing (array of indices) instead of direct indexing
                cluster_edit_matrix = all_edits[cluster_dedup_indices, :]
                # Get positions that have ANY edits in this cluster
                edited_pos = np.where(np.sum(cluster_edit_matrix, axis=0) > 0)[0]
                cluster_edited_positions[cluster_id] = set(edited_pos)
        
        logger.info(f"Extracted edit patterns for {len(cluster_edited_positions)} clusters")
        # Consensus analysis with majority voting and masking
        current_locus_start = ref_range_dict[locus][1]
        run_consensus_analysis(sample_name, locus, base_dir, sample_dir, 
                            fastq_file, ref_range_dict, current_locus_start, 
                            logger, n_cores=n_cores_for_consensus,
                            primer_seq=primer_seq, ref_seqs=ref_seqs,
                            cluster_edited_positions=cluster_edited_positions)
        
        # Plotting
        logger.info("Generating plots...")
        min_num = 9500
        all_deduped_inds = ddda_data.deduped_inds[locus]
        total_deduped_reads = len(all_deduped_inds)
        
        if total_deduped_reads >= min_num:
            undel_read_inds = np.random.choice(all_deduped_inds, min_num, replace=False)
        else:
            undel_read_inds = all_deduped_inds
            
        selected_read_inds = undel_read_inds
        strands = ddda_data.read_strands[locus][selected_read_inds]
        CtoT_inds = selected_read_inds[strands == 0]
        GtoA_inds = selected_read_inds[strands == 1]
        
        CtoT_edits = np.array(ddda_data.edit_dict[locus][CtoT_inds, :].todense())
        GtoA_edits = np.array(ddda_data.edit_dict[locus][GtoA_inds, :].todense())
        edits = np.array(ddda_data.edit_dict[locus][selected_read_inds, :].todense())
        
        track_ac = np.mean(edits, axis=0)
        track_ac_smoothed = np.convolve(track_ac, np.ones(50), mode='same') / 50
        
        plot_range = np.arange(500, len(track_ac_smoothed) - 500)
        
        fig = plt.figure(figsize=(8, 6), dpi=200)
        gs = gridspec.GridSpec(4, 1, height_ratios=[0.5, 2, 2, 2])
        
        ax0 = plt.subplot(gs[0])
        x_values = np.arange(len(track_ac_smoothed[plot_range]))
        ax0.fill_between(x_values, track_ac_smoothed[plot_range], color='purple', alpha=1)
        ax0.set_xlim([x_values.min(), x_values.max()])
        ax0.set_ylim([0, 0.1])
        ax0.set_title(f'{sample_name} TDAC-seq')
        ax0.set_ylabel('DddA\nMutation\nRate', fontsize=8)
        
        ax1 = plt.subplot(gs[1])
        read_edit_num = np.sum(CtoT_edits, axis=1)
        row_order = [i for i in np.argsort(-read_edit_num)]
        ax1.imshow(CtoT_edits[row_order, :][:, plot_range], aspect='auto', vmax=0.7, vmin=0, cmap=custom_cmap_blue)
        ax1.set_ylabel('CtoT edits')
        
        ax2 = plt.subplot(gs[2])
        read_edit_num = np.sum(GtoA_edits, axis=1)
        row_order = [i for i in np.argsort(-read_edit_num)]
        ax2.imshow(GtoA_edits[row_order, :][:, plot_range], aspect='auto', vmax=0.7, vmin=0, cmap=custom_cmap_red)
        ax2.set_ylabel('GtoA edits')
        
        ax3 = plt.subplot(gs[3])
        read_edit_num = np.sum(edits, axis=1)
        row_order = [i for i in np.argsort(-read_edit_num)]
        ax3.imshow(edits[row_order, :][:, plot_range], aspect='auto', vmax=0.5, vmin=0, cmap=custom_cmap_purple)
        ax3.set_xlabel('Region')
        ax3.set_ylabel('All edits')
        
        for ax in [ax0, ax1, ax2, ax3]:
             ax.spines['top'].set_visible(False)
             ax.spines['right'].set_visible(False)
             ax.set_xticks([])
             if ax != ax0: ax.set_yticks([])
        
        plt.tight_layout()
        plt.subplots_adjust(hspace=0.05)
        
        fig_path = f'{figure_dir}/{sample_name}_tdac_analysis.pdf'
        plt.savefig(fig_path, format='pdf', bbox_inches='tight')
        plt.close()
        logger.info(f"Saved: {fig_path}")
        
        logger.info(f"\n{'='*60}\nâœ… Sample {sample_name} complete\n{'='*60}\n")
        
        return {
            'sample_name': sample_name,
            'accessibility': track_ac_smoothed,
            'ddda_data': ddda_data,
            'status': 'success'
        }
    
    except Exception as e:
        logger.error(f"ERROR processing {sample_name}: {str(e)}", exc_info=True)
        return {
            'sample_name': sample_name,
            'accessibility': None,
            'ddda_data': None,
            'status': 'failed',
            'error': str(e)
        }




# ==============================================================================
# === MAIN PIPELINE ============================================================
# ==============================================================================




if __name__ == "__main__":
    
    parser = argparse.ArgumentParser(description="Run TDAC-seq pipeline (parallelized)")
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('--pod5', type=str, help="Path to pod5 folder")
    group.add_argument('--bam', type=str, help="Path to basecalled BAM")
    group.add_argument('--pod5-archive', type=str, help="Path to pod5 .tar.gz archive")
    
    args = parser.parse_args()
    
    if args.pod5:
        start_type = "pod5"
        input_path = os.path.abspath(args.pod5)
    elif args.bam:
        start_type = "bam"
        input_path = os.path.abspath(args.bam)
    elif args.pod5_archive:
        start_type = "archive"
        input_path = os.path.abspath(args.pod5_archive)
    
    # Run preprocessing
    run_preprocessing_shell(base_dir, RUN_TDAC_SH, genome_file, start_type, input_path)

    # Load samples
    samples = load_sample_config(base_dir)
    sample_list = list(samples.items())
    
    N_CORES_TOTAL = multiprocessing.cpu_count()
    N_CORES_FOR_CONSENSUS = max(1, N_CORES_TOTAL - 2)
    
    print(f"\n{'='*60}")
    print(f"ðŸš€ SERIAL SAMPLE PROCESSING")
    print(f"{'='*60}")
    print(f"Total Cores: {N_CORES_TOTAL}")
    print(f"Samples to process: {len(sample_list)}")
    print(f"Consensus Cores: {N_CORES_FOR_CONSENSUS}")
    print(f"Consensus Method: Majority Voting with Clustering-Based Masking")
    print(f"Primer Sequence: {PRIMER_SEQ}")
    print(f"Masking: Positions identified by clustering deamination pattern")
    print(f"{'='*60}\n")
    
    # Load reference ONCE at the start
    print("Loading reference FASTA...")
    ref_fa_path = os.path.join(base_dir, "ref.fa")
    
# Verify file exists
    if not os.path.exists(ref_fa_path):
        print(f"ERROR: ref.fa not found at {ref_fa_path}")
        sys.exit(1)
    
    # Read the single sequence (assume only one sequence in file)
    ref_seqs = {}
    try:
        with open(ref_fa_path, 'r') as f:
            current_seq = ""
            locus_name = None
            
            for line in f:
                line = line.strip()
                if not line:
                    continue
                
                if line.startswith('>'):
                    # Extract locus name from header
                    locus_name = line[1:].split()[0]  # Remove '>' and take first word
                else:
                    current_seq += line
            
            # Store the single sequence
            if locus_name and current_seq:
                ref_seqs[locus_name] = current_seq.upper()
        
        if not ref_seqs:
            print(f"ERROR: Could not read sequence from {ref_fa_path}")
            sys.exit(1)
        
        # Get the sequence (there's only one)
        seq_name = list(ref_seqs.keys())[0]
        seq_len = len(ref_seqs[seq_name])
        
        print(f"âœ“ Loaded reference: {seq_name} ({seq_len} bp)")
    
    except Exception as e:
        print(f"ERROR loading reference: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    # Create partial function with fixed arguments
    worker_func = partial(
        process_single_sample,
        base_dir=base_dir,
        ref_range_dict=ref_range_dict,
        ref_seqs=ref_seqs,
        genome_file=genome_file,
        bias_model=bias_model,
        locus=locus,
        figure_dir=figure_dir,
        data_dir=data_dir,
        run_prefix=RUN_PREFIX,
        n_cores_for_consensus=N_CORES_FOR_CONSENSUS,
        primer_seq=PRIMER_SEQ
    )
    print(f"[*] Starting serial processing...")
    results = []
    for sample_data in sample_list:
        results.append(worker_func(sample_data))
    
    # Aggregate results
    print(f"\n{'='*60}")
    print(f"Aggregating results...")
    print(f"{'='*60}\n")
    
    all_accessibility = {}
    processed_data = {}
    failed_samples = []
    
    for result in results:
        sample_name = result['sample_name']
        if result['status'] == 'success':
            all_accessibility[sample_name] = result['accessibility']
            processed_data[sample_name] = result['ddda_data']
            print(f"âœ“ {sample_name}")
        else:
            failed_samples.append(sample_name)
            print(f"âœ— {sample_name}: {result.get('error', 'Unknown error')}")
    
    print(f"\nProcessed: {len(processed_data)} samples")
    if failed_samples:
        print(f"Failed: {len(failed_samples)} samples - {failed_samples}")
    
    # Combined plot
    if not all_accessibility:
        print("\n[ERROR] No successful samples processed. Exiting.")
        sys.exit(1)
    
    print(f"\n{'='*60}\nGenerating combined plot\n{'='*60}")

    plot_range = np.arange(500, len(list(all_accessibility.values())[0]) - 500)
    all_values = [v for track in all_accessibility.values() for v in track[plot_range]]
    y_min, y_max = 0, max(all_values) * 1.1

    n_samples = len(processed_data)
    fig_height = 2 + (n_samples * 2)

    fig = plt.figure(figsize=(12, fig_height), dpi=200)
    gs = gridspec.GridSpec(n_samples + 1, 1, height_ratios=[1] + [2] * n_samples)

    ax0 = plt.subplot(gs[0])
    color_palette = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']
    sample_colors = {}
    for idx, sample in enumerate(sorted(all_accessibility.keys())):
        sample_colors[sample] = color_palette[idx % len(color_palette)]

    for sample, track in all_accessibility.items():
        ax0.plot(plot_range, track[plot_range], 
                 label=sample, color=sample_colors[sample],
                 linewidth=1.5, alpha=0.8)

    ax0.set_ylabel('DddA\nAccessibility', fontsize=10)
    ax0.set_ylim([y_min, y_max]) 
    ax0.set_xlim([plot_range.min(), plot_range.max()])
    ax0.set_title('Combined TDAC-seq Analysis', fontsize=12)
    ax0.legend(loc='upper right', fontsize=9)
    ax0.spines['top'].set_visible(False)
    ax0.spines['right'].set_visible(False)
    ax0.set_xticks([])

    for idx, sample in enumerate(sorted(processed_data.keys())):
        ddda_data = processed_data[sample]
        min_num = 8000 
        
        all_deduped_inds = ddda_data.deduped_inds[locus]
        total_deduped_reads = len(all_deduped_inds)
        
        if total_deduped_reads >= min_num:
            undel_read_inds = np.random.choice(all_deduped_inds, min_num, replace=False)
        else:
            undel_read_inds = all_deduped_inds
        
        edits = np.array(ddda_data.edit_dict[locus][undel_read_inds, :].todense())
        read_edit_num = np.sum(edits, axis=1)
        row_order = [i for i in np.argsort(-read_edit_num)]
        
        ax = plt.subplot(gs[idx + 1])
        ax.imshow(edits[row_order, :][:, plot_range], 
                  aspect='auto', vmax=0.5, vmin=0, cmap=custom_cmap_purple)
        ax.set_ylabel(f'{sample}\nEdits', fontsize=9)
        ax.set_yticks([])
        
        if idx < n_samples - 1:
            ax.set_xticks([])
        else:
            ax.set_xlabel('Genomic position', fontsize=10)
            ax.set_xticks([])

    plt.tight_layout()
    plt.subplots_adjust(hspace=0.05)

    combined_path = f'{figure_dir}/combined_analysis.pdf'
    plt.savefig(combined_path, format='pdf', bbox_inches='tight')
    plt.close()
    print(f"âœ“ Saved: {combined_path}")
    
    print(f"\n{'='*60}\nâœ… PIPELINE COMPLETE\n{'='*60}")
    print(f"âœ“ Processed {len(processed_data)} samples")
    print(f"âœ“ Figures saved to: {figure_dir}")
    print(f"âœ“ Data saved to: {data_dir}")
    print(f"{'='*60}\n")
