#!/usr/bin/env python3
"""
TDAC-seq processing pipeline: Preprocessing, alignment, deduplication,
consensus variant calling with QC validation, and plotting.
"""
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import pickle
from tdac_seq.ddda_dataset import ddda_dataset
from tdac_seq.utils import export_to_tsv
from matplotlib.colors import ListedColormap
import os
import sys 
import logging
import subprocess 
import argparse
import mappy
import pysam
import spoa
from collections import defaultdict
import re
import multiprocessing
from functools import partial



# ==================== Configuration ====================



SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
base_dir = os.path.dirname(SCRIPT_DIR)



RUN_TDAC_SH = os.path.join(SCRIPT_DIR, "basecall_demultiplex.sh")
RUN_PREFIX = "DS98"



genome_file = "/home/dan/Index/GRCh38.primary_assembly.genome.fa" 
bias_model_path = os.path.join(base_dir, "data/bias_dict.pkl")



ref_range_dict = {
    'HCT_distal': ('chr8', 129706074, 129713000)
}
locus = "HCT_distal" 



figure_dir = os.path.join(base_dir, "figures")
data_dir = os.path.join(base_dir, "data") 
os.makedirs(figure_dir, exist_ok=True)
os.makedirs(data_dir, exist_ok=True)



# Load bias model
try:
    with open(bias_model_path, "rb") as f:
        bias_model = pickle.load(f)
except FileNotFoundError:
    print(f"ERROR: Bias model not found at {bias_model_path}")
    sys.exit(1)



# Custom colormaps
base_colors_blue = plt.cm.Blues(np.linspace(0, 1, 256))
base_colors_blue[0] = [0.97, 0.97, 0.97, 1]
base_colors_blue[-1] = [0, 0, 0.5, 1]
custom_cmap_blue = ListedColormap(base_colors_blue)



base_colors_red = plt.cm.Reds(np.linspace(0, 1, 256))
base_colors_red[0] = [0.97, 0.97, 0.97, 1]
base_colors_red[-1] = [0.5, 0, 0, 1]
custom_cmap_red = ListedColormap(base_colors_red)



base_colors_purple = plt.cm.Purples(np.linspace(0, 1, 256))
base_colors_purple[0] = [0.97, 0.97, 0.97, 1]
base_colors_purple[-1] = [0.25, 0, 0.5, 1]
custom_cmap_purple = ListedColormap(base_colors_purple)



# ==============================================================================
# === PREPROCESSING FUNCTIONS ==================================================
# ==============================================================================



def run_preprocessing_shell(base_dir, run_tdac_sh, genome_file, start_type, input_path):
    """Executes basecall_demultiplex.sh for preprocessing."""
    print(f"\n{'='*60}")
    print(f"ðŸš€ Launching TDAC Preprocessing: {os.path.basename(run_tdac_sh)}")
    print(f"  Project Root: {base_dir}")
    print(f"  Genome Path: {genome_file}") 
    print(f"  Start Type: {start_type}")
    print(f"  Input Path: {input_path}")
    print(f"{'='*60}")
    
    try:
        command = [
            "mamba", "run", "-n", "nano", "bash", 
            run_tdac_sh, base_dir, genome_file, start_type, input_path
        ]
        result = subprocess.run(command, check=True, capture_output=False, text=True)
        print(f"\n Preprocessing complete. Exit code: {result.returncode}")
    except subprocess.CalledProcessError as e:
        print(f"\n ERROR: Shell script failed with exit code {e.returncode}")
        print(f"Stderr:\n{e.stderr}")
        sys.exit(1)
    except FileNotFoundError:
        print(f"\n ERROR: basecall_demultiplex.sh not found at {run_tdac_sh}")
        sys.exit(1)



def load_sample_config(base_dir):
    """
    Load unique samples and point to their MERGED FASTQ files.
    """
    rename_table_path = os.path.join(base_dir, "data", "rename_table.tsv")
    if not os.path.exists(rename_table_path):
        print(f"ERROR: Rename table not found at {rename_table_path}")
        print("This should be generated by basecall_demultiplex.sh")
        sys.exit(1)
        
    df_rename = pd.read_csv(rename_table_path, sep='\t')
    
    sample_dict = {}
    # RUN_PREFIX and PER_SAMPLE_DIR must match your shell script
    RUN_PREFIX = "DS98" 
    PER_SAMPLE_DIR = os.path.join(base_dir, "data", "per_sample")
    
    # Get unique samples from rename table
    for sample_name in df_rename['Sample'].unique():
        # Reconstruct the exact path to the merged file
        merged_fastq = os.path.join(
            PER_SAMPLE_DIR,
            f"{RUN_PREFIX}_{sample_name}.fastq.gz"
        )
        
        if os.path.exists(merged_fastq):
            sample_dict[sample_name] = merged_fastq
        else:
            print(f"WARNING: Merged FASTQ not found for {sample_name}: {merged_fastq}")
    
    print(f"\n[CONFIG] Loaded {len(sample_dict)} unique samples:")
    for name, path in sample_dict.items():
        print(f"  âœ“ {name}: {os.path.basename(path)}")
    
    if not sample_dict:
        print("\n[ERROR] No valid samples found!")
        sys.exit(1)
    
    return sample_dict



# ==============================================================================
# === CONSENSUS VALIDATION FUNCTIONS ==========================================
# ==============================================================================



def load_reference_region(ref_fa_path, chrom, start, end):
    """
    Load reference sequence for a specific region.
    Returns uppercase reference sequence or None on error.
    """
    try:
        with pysam.FastaFile(ref_fa_path) as fasta:
            ref_seq = fasta.fetch(chrom, start, end).upper()
            return ref_seq
    except Exception as e:
        print(f"ERROR loading reference region {chrom}:{start}-{end}: {e}")
        return None


def validate_consensus_against_ref(consensus_seq, ref_seq, aligner, cluster_id, logger=None):
    """
    Align consensus to reference and compute error metrics.
    
    Args:
        consensus_seq (str): consensus sequence from SPOA
        ref_seq (str): reference sequence for the region
        aligner: mappy.Aligner object
        cluster_id: cluster identifier
        logger: logging object
    
    Returns:
        dict with alignment quality metrics
    """
    if not consensus_seq or not ref_seq:
        return {
            'identity': 0, 
            'n_mismatches': 0, 
            'n_insertions': 0,
            'n_deletions': 0,
            'alignment_length': 0,
            'status': 'empty_sequence'
        }
    
    try:
        # Map consensus to reference
        hits = list(aligner.map(consensus_seq, cs=True))
        
        if not hits:
            if logger:
                logger.warning(f"Cluster {cluster_id}: consensus did not align to reference")
            return {
                'identity': 0, 
                'n_mismatches': 0, 
                'n_insertions': 0,
                'n_deletions': 0,
                'alignment_length': 0,
                'status': 'no_hit'
            }
        
        hit = hits[0]
        
        # Parse cs tag: :matches*ref_alt+insertions-deletions
        tokens = re.findall(r'(:[0-9]+|\*[a-z][a-z]|\+[a-z]+|\-[a-z]+)', hit.cs)
        
        mismatches = 0
        insertions = 0
        deletions = 0
        matches = 0
        
        for token in tokens:
            op = token[0]
            val = token[1:]
            
            if op == ':':
                matches += int(val)
            elif op == '*':
                mismatches += 1
            elif op == '+':
                insertions += len(val)
            elif op == '-':
                deletions += len(val)
        
        # Calculate identity on consensus length
        consensus_len = len(consensus_seq)
        aligned_matches = matches
        identity_pct = (aligned_matches / consensus_len * 100) if consensus_len > 0 else 0
        
        total_aligned = matches + mismatches + insertions
        
        qc_dict = {
            'identity': round(identity_pct, 2),
            'n_mismatches': mismatches,
            'n_insertions': insertions,
            'n_deletions': deletions,
            'matches': matches,
            'alignment_length': total_aligned,
            'consensus_length': consensus_len,
            'ref_start': hit.r_st,
            'ref_end': hit.r_en,
            'query_start': hit.q_st,
            'query_end': hit.q_en,
            'mapping_quality': hit.mapq,
            'status': 'aligned'
        }
        
        # Log if poor quality
        if identity_pct < 90:
            if logger:
                logger.warning(f"Cluster {cluster_id}: LOW consensus identity={identity_pct:.1f}%, "
                               f"mismatches={mismatches}, indels={insertions+deletions}")
        
        return qc_dict
        
    except Exception as e:
        if logger:
            logger.error(f"Cluster {cluster_id}: Error validating consensus: {e}")
        return {
            'identity': 0,
            'n_mismatches': 0,
            'n_insertions': 0,
            'n_deletions': 0,
            'alignment_length': 0,
            'status': 'error',
            'error_msg': str(e)
        }



# ==============================================================================
# === CONSENSUS SCRIPT FUNCTIONS ===============================================
# ==============================================================================



def get_variants_from_local_ref(aligner, consensus_seq, cluster_id, offset):
    """Aligns consensus sequence and returns all variants."""
    try:
        hits = list(aligner.map(consensus_seq, cs=True))
    except Exception:
        return []
    
    if not hits: 
        return []
    
    hit = hits[0]
    curr_ref_relative = hit.r_st
    variants = []
    
    tokens = re.findall(r'(:[0-9]+|\*[a-z][a-z]|\+[a-z]+|\-[a-z]+)', hit.cs)
    
    for token in tokens:
        op, val = token[0], token[1:]
        
        if op == ':':
            curr_ref_relative += int(val)
        elif op == '*':
            ref_base, alt_base = val[0].upper(), val[1].upper()
            genomic_pos = curr_ref_relative + offset
            variants.append({
                'Cluster_ID': cluster_id,
                'Genomic_Pos': genomic_pos,
                'Ref': ref_base,
                'Alt': alt_base,
                'Type': 'SNP'
            })
            curr_ref_relative += 1
        elif op == '-':
            # Deletion in consensus
            curr_ref_relative += len(val)
        elif op == '+':
            # Insertion in consensus
            genomic_pos = curr_ref_relative + offset
            variants.append({
                'Cluster_ID': cluster_id,
                'Genomic_Pos': genomic_pos,
                'Ref': '-',
                'Alt': val.upper(),
                'Type': 'INSERTION'
            })
            
    return variants



def process_cluster(cluster_tuple, ref_fa_path, ref_seq, chrom, start_pos, offset, logger=None):
    """
    Generate consensus via majority voting against reference (not SPOA).
    """
    import mappy
    
    cluster_id, read_data = cluster_tuple
    
    try:
        read_names, sequences = zip(*read_data)
        read_names = list(read_names)
        sequences = list(sequences)
    except ValueError:
        return {
            'variants': [], 
            'msa_string': '', 
            'cluster_id': cluster_id, 
            'cluster_size': 0,
            'consensus_qc': {'status': 'empty_cluster'}
        }
    
    cluster_size = len(sequences)
    
    # Initialize aligner
    try:
        aligner = mappy.Aligner(ref_fa_path, preset="map-ont")
        if not aligner:
            return {'variants': [], 'msa_string': '', 'cluster_id': cluster_id, 
                    'cluster_size': 0, 'consensus_qc': {'status': 'aligner_init_failed'}}
    except Exception as e:
        return {'variants': [], 'msa_string': '', 'cluster_id': cluster_id, 
                'cluster_size': 0, 'consensus_qc': {'status': 'aligner_error'}}
    
    # ALIGN each read to reference independently
    pileup = {}  # position -> {base -> count}
    read_alignments = []
    
    for read_name, sequence in zip(read_names, sequences):
        try:
            hits = list(aligner.map(sequence, cs=True))
            if not hits:
                if logger:
                    logger.warning(f"Cluster {cluster_id}: Read {read_name[:16]}... did not align")
                continue
            
            hit = hits[0]  # Best hit
            
            # Parse the alignment
            tokens = re.findall(r'(:[0-9]+|\*[a-z][a-z]|\+[a-z]+|\-[a-z]+)', hit.cs)
            
            curr_ref_pos = hit.r_st
            curr_query_pos = 0
            
            for token in tokens:
                op = token[0]
                val = token[1:]
                
                if op == ':':  # Matches
                    length = int(val)
                    for i in range(length):
                        ref_pos = curr_ref_pos + i
                        query_base = sequence[curr_query_pos + i]
                        
                        if ref_pos not in pileup:
                            pileup[ref_pos] = {}
                        pileup[ref_pos][query_base] = pileup[ref_pos].get(query_base, 0) + 1
                    
                    curr_ref_pos += length
                    curr_query_pos += length
                
                elif op == '*':  # Mismatch
                    ref_base = val[0].upper()
                    alt_base = val[1].upper()
                    
                    if curr_ref_pos not in pileup:
                        pileup[curr_ref_pos] = {}
                    pileup[curr_ref_pos][alt_base] = pileup[curr_ref_pos].get(alt_base, 0) + 1
                    
                    curr_ref_pos += 1
                    curr_query_pos += 1
                
                elif op == '+':  # Insertion (skip in consensus)
                    curr_query_pos += len(val)
                
                elif op == '-':  # Deletion (skip)
                    curr_ref_pos += len(val)
        
        except Exception as e:
            if logger:
                logger.error(f"Cluster {cluster_id}: Error aligning {read_name}: {e}")
            continue
    
    # MAJORITY VOTE: call consensus at each position
    consensus_bases = []
    consensus_positions = sorted(pileup.keys())
    
    for pos in consensus_positions:
        bases_at_pos = pileup[pos]
        majority_base = max(bases_at_pos.items(), key=lambda x: x[1])[0]
        consensus_bases.append(majority_base)
    
    consensus_seq = ''.join(consensus_bases)
    
    if len(consensus_seq) == 0:
        return {'variants': [], 'msa_string': '', 'cluster_id': cluster_id, 
                'cluster_size': cluster_size, 'consensus_qc': {'status': 'no_consensus'}}
    
    # Validate consensus against reference
    consensus_qc = validate_consensus_against_ref(
        consensus_seq, ref_seq, aligner, cluster_id, logger=logger
    )
    
    # Get variants
    variants = get_variants_from_local_ref(aligner, consensus_seq, cluster_id, offset)
    
    for v in variants:
        v['Cluster_Size'] = cluster_size
        v['Consensus_Identity'] = consensus_qc.get('identity', np.nan)
    
    return {
        'variants': variants, 
        'msa_string': f">consensus_{cluster_id}\n{consensus_seq}\n>reference\n{ref_seq}", 
        'cluster_id': cluster_id,
        'cluster_size': cluster_size,
        'consensus': consensus_seq,
        'consensus_qc': consensus_qc
    }


def run_consensus_analysis(sample_name, locus, base_dir, sample_dir, fastq_path, 
                         ref_range_dict, target_start_offset, logger=None, n_cores=None):
    """Parallel consensus/variant calling with QC validation."""
    if logger is None:
        logger = logging.getLogger(sample_name)
        logger.handlers = []
        logger.setLevel(logging.INFO)
        log_path = os.path.join(sample_dir, f"{sample_name}_consensus.log")
        fh = logging.FileHandler(log_path, mode='w')
        fh.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))
        logger.addHandler(fh)
    
    logger.info(f"\n{'-'*50}")
    logger.info(f"Consensus Variant Analysis for {sample_name}")
    logger.info(f"{'-'*50}")
    
    grouping_path = f"{sample_dir}/{sample_name}_grouping.tsv"
    ref_fa_path = os.path.join(base_dir, "ref.fa") 
    output_csv = f"{sample_dir}/{sample_name}_cluster_consensus_TRUE_variants.csv"
    qc_output_csv = f"{sample_dir}/{sample_name}_consensus_qc_report.csv"
    msa_output_path = f"{sample_dir}/{sample_name}_cluster_msa.fasta"
    min_coverage = 4
    
    N_CORES = max(1, n_cores if n_cores else (multiprocessing.cpu_count() - 2))
    
    # Check required files exist
    if not all(os.path.exists(p) for p in [grouping_path, ref_fa_path, fastq_path]):
        logger.error("[ERROR] Missing required files (grouping.tsv, ref.fa, or fastq)")
        return
    
    # LOAD REFERENCE SEQUENCE ONCE before spawning workers
    logger.info(f"Loading reference region from {ref_fa_path}...")
    try:
        with pysam.FastaFile(ref_fa_path) as fasta:
            ref_seq = fasta.fetch(locus).upper()
        logger.info(f"Loaded {len(ref_seq)} bp reference sequence")
    except Exception as e:
        logger.error(f"Failed to load reference: {e}")
        return
    
    # Extract reference region info (for offset calculations)
    chrom, start, end = ref_range_dict[locus]
    
    # Load grouping and build clusters
    df_groups = pd.read_csv(grouping_path, sep='\t', header=None, names=['Read_ID', 'Cluster_ID'])
    cluster_map = dict(zip(df_groups['Read_ID'], df_groups['Cluster_ID']))
    
    # Store (read_name, sequence) tuples
    clusters = defaultdict(list)
    logger.info(f"Indexing FASTQ...")
    with pysam.FastxFile(fastq_path) as fh:
        for entry in fh:
            if entry.name in cluster_map:
                c_id = cluster_map[entry.name]
                clusters[c_id].append((entry.name, entry.sequence))
    
    tasks = [(c_id, read_data) for c_id, read_data in clusters.items() if len(read_data) >= min_coverage]
    logger.info(f"Processing {len(tasks)} clusters (>={min_coverage} reads) on {N_CORES} cores...")
    
    # Create worker function
    worker_func = partial(
        process_cluster, 
        ref_fa_path=ref_fa_path, 
        ref_seq=ref_seq,
        chrom=chrom,
        start_pos=start,
        offset=target_start_offset,
        logger=logger
    )
    
    # Run consensus calling in parallel
    with multiprocessing.Pool(N_CORES) as pool:
        results = pool.map(worker_func, tasks) 
    
    # Collect results
    all_variants_raw = []
    all_qc_metrics = []
    logger.info(f"Collecting results and writing MSAs to {msa_output_path}...")
    
    with open(msa_output_path, 'w') as msa_f:
        for res_dict in results:
            all_variants_raw.extend(res_dict['variants'])
            
            # Store QC metrics per cluster
            qc_row = {
                'Cluster_ID': res_dict['cluster_id'],
                'Cluster_Size': res_dict['cluster_size'],
                'Consensus_Status': res_dict['consensus_qc'].get('status', 'unknown'),
                'Identity_Pct': res_dict['consensus_qc'].get('identity', np.nan),
                'Mismatches': res_dict['consensus_qc'].get('n_mismatches', np.nan),
                'Insertions': res_dict['consensus_qc'].get('n_insertions', np.nan),
                'Deletions': res_dict['consensus_qc'].get('n_deletions', np.nan),
                'Num_Variants_Called': len(res_dict['variants'])
            }
            all_qc_metrics.append(qc_row)
            
            if res_dict['msa_string']:
                msa_f.write(f"# === CLUSTER {res_dict['cluster_id']} (Size={res_dict['cluster_size']}, "
                           f"Identity={res_dict['consensus_qc'].get('identity', 'N/A')}%) ===\n")
                msa_f.write(res_dict['msa_string'])
                msa_f.write("\n\n")
    
    # Save QC report
    df_qc = pd.DataFrame(all_qc_metrics)
    df_qc.to_csv(qc_output_csv, index=False)
    logger.info(f"âœ“ Saved QC report: {qc_output_csv}")
    
    # Flag low-quality consensuses
    low_quality = df_qc[df_qc['Identity_Pct'] < 90]
    logger.info(f"\nQC Summary: {len(low_quality)}/{len(df_qc)} clusters with identity <90%")
    
    if len(low_quality) > 0:
        logger.info("Low-quality clusters (Identity <90%):")
        for _, row in low_quality.iterrows():
            logger.info(f"  Cluster {int(row['Cluster_ID'])}: Identity={row['Identity_Pct']:.1f}%, "
                       f"Size={int(row['Cluster_Size'])}, Mismatches={int(row['Mismatches'])}, "
                       f"Indels={int(row['Insertions']+row['Deletions'])}")
    
    # Process variants
    if all_variants_raw:
        df_raw = pd.DataFrame(all_variants_raw)
        
        # Filter deamination artifacts
        is_C_to_T = (df_raw['Ref'] == 'C') & (df_raw['Alt'] == 'T')
        is_G_to_A = (df_raw['Ref'] == 'G') & (df_raw['Alt'] == 'A')
        df_filtered = df_raw[~(is_C_to_T | is_G_to_A)]
        
        df_filtered.to_csv(output_csv, index=False)
        n_deamination = len(df_raw) - len(df_filtered)
        logger.info(f"\nVariant Summary:")
        logger.info(f"  Total called: {len(df_raw)}")
        logger.info(f"  Deamination artifacts (Câ†’T/Gâ†’A): {n_deamination}")
        logger.info(f"  True variants: {len(df_filtered)}")
        
        if not df_filtered.empty:
            logger.info(f"\nTop 10 Variants:")
            summary = df_filtered.groupby(['Genomic_Pos', 'Ref', 'Alt']).size().rename('Count').sort_values(ascending=False).head(10)
            for (pos, ref, alt), count in summary.items():
                logger.info(f"  {pos}: {ref}â†’{alt} (n={count})")
    else:
        logger.info("No variants found")
    
    logger.info(f"\n{'-'*50}\nâœ… Finished Consensus Analysis\n{'-'*50}\n")



# ==============================================================================
# === (NOW SERIAL) WORKER FUNCTION =============================================
# ==============================================================================



def process_single_sample(sample_data, base_dir, ref_range_dict, genome_file, 
                         bias_model, locus, figure_dir, data_dir, run_prefix,
                         n_cores_for_consensus):
    """
    Function to process a single sample.
    This is now called SERIALLY from the main thread.
    """
    sample_name, fastq_file = sample_data
    sample_dir = os.path.join(data_dir, sample_name)
    os.makedirs(sample_dir, exist_ok=True)
    
    log_path = os.path.join(sample_dir, f"{sample_name}_processing.log")
    logger = logging.getLogger(sample_name)
    logger.handlers = []
    logger.setLevel(logging.INFO)
    fh = logging.FileHandler(log_path, mode='w')
    fh.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))
    logger.addHandler(fh)
    
    logger.info(f"\n{'='*60}\nProcessing: {sample_name}\n{'='*60}")
    
    try:
        # Initialize dataset
        ddda_data = ddda_dataset(
            ID=sample_name, 
            region_dict=ref_range_dict, 
            genome_file=genome_file, 
            fastq_file=fastq_file, 
            working_dir=base_dir
        )
        ddda_data.bias_model = bias_model
        
        # Align
        logger.info("Aligning reads...")
        # This function creates its OWN internal pool (as seen in traceback)
        # This is now SAFE because process_single_sample is in the main thread.
        ddda_data.align_reads(start_gap_threshold=500, end_gap_threshold=500)
        n_mapped = len(ddda_data.read_ids[locus])
        logger.info(f"MAPPED READS: {n_mapped}")

        logger.info("Exporting mapped reads...")

        # Export mapped read IDs with their alignment info
        mapped_read_ids = ddda_data.read_ids[locus]
        mapped_output = os.path.join(sample_dir, f"{sample_name}_mapped_reads.txt")
        with open(mapped_output, 'w') as f:
            for read_id in mapped_read_ids:
                f.write(f"{read_id}\n")
        logger.info(f"  âœ“ Exported {len(mapped_read_ids)} mapped read IDs to {mapped_output}")

        # If ddda_data has mapping positions, export those too
        if hasattr(ddda_data, 'read_positions') and locus in ddda_data.read_positions:
            positions = ddda_data.read_positions[locus]
            strands = ddda_data.read_strands[locus] if hasattr(ddda_data, 'read_strands') else None
            
            mapping_output = os.path.join(sample_dir, f"{sample_name}_read_mapping.tsv")
            with open(mapping_output, 'w') as f:
                f.write("Read_ID\tPosition\tStrand\n")
                for i, read_id in enumerate(mapped_read_ids):
                    strand = strands[i] if strands is not None else "."
                    f.write(f"{read_id}\t{positions[i]}\t{strand}\n")
            logger.info(f"  âœ“ Exported mapping positions to {mapping_output}")


        # Export TSVs
        logger.info("Exporting TSVs...")
        export_to_tsv(ddda_data, export_DddA_edit=True, export_del=False, 
                     export_ABE_edit=False, export_strand=True, 
                     export_dir=sample_dir, overwrite=False)
        
        # Deduplication
        logger.info("Deduplication...")
        grouping_path = f"{sample_dir}/{sample_name}_grouping.tsv"
        cluster_json_path = f"{sample_dir}/{sample_name}_clusters.json"
        ddda_data.dedup_all(
            grouping_export_path=grouping_path,
            cluster_data_export_json_path=cluster_json_path
        )
        
        # Fix grouping file
        logger.info("Converting indices to Read IDs...")
        if os.path.exists(grouping_path):
            df_grouping = pd.read_csv(grouping_path, sep='\t', header=None, names=['Read_Idx', 'Group_Idx'])
            actual_read_ids = np.array(ddda_data.read_ids[locus])
            if pd.api.types.is_integer_dtype(df_grouping['Read_Idx']):
                df_grouping['Read_ID'] = actual_read_ids[df_grouping['Read_Idx']]
                df_final = df_grouping[['Read_ID', 'Group_Idx']]
                df_final.to_csv(grouping_path, sep='\t', index=False, header=False)
                logger.info("  âœ“ Grouping file fixed")
        
        # Save deduplicated reads
        final_deduplicated_indices = ddda_data.deduped_inds[locus]
        final_list_path = f"{sample_dir}/{sample_name}_final_deduped_readIDs.txt"
        all_read_ids = np.array(ddda_data.read_ids[locus])
        final_read_names = all_read_ids[final_deduplicated_indices]
        np.savetxt(final_list_path, final_read_names, fmt='%s')
        
        n_deduped = len(final_deduplicated_indices)
        percent_kept = (n_deduped / n_mapped * 100) if n_mapped > 0 else 0
        logger.info(f"DEDUPLICATED: {n_deduped} ({percent_kept:.2f}% retained)")


        # Save pickle
        pkl_path = f"{sample_dir}/ddda_data_{sample_name}.pkl"
        with open(pkl_path, 'wb') as f:
            pickle.dump(ddda_data, f)
        
        # Consensus analysis with QC
        current_locus_start = ref_range_dict[locus][1]
        run_consensus_analysis(sample_name, locus, base_dir, sample_dir, 
                              fastq_file, ref_range_dict, current_locus_start, 
                              logger, n_cores=n_cores_for_consensus)
        
        # Plotting
        logger.info("Generating plots...")
        min_num = 9500
        all_deduped_inds = ddda_data.deduped_inds[locus]
        total_deduped_reads = len(all_deduped_inds)
        
        if total_deduped_reads >= min_num:
            undel_read_inds = np.random.choice(all_deduped_inds, min_num, replace=False)
        else:
            undel_read_inds = all_deduped_inds
            
        selected_read_inds = undel_read_inds
        strands = ddda_data.read_strands[locus][selected_read_inds]
        CtoT_inds = selected_read_inds[strands == 0]
        GtoA_inds = selected_read_inds[strands == 1]
        
        CtoT_edits = np.array(ddda_data.edit_dict[locus][CtoT_inds, :].todense())
        GtoA_edits = np.array(ddda_data.edit_dict[locus][GtoA_inds, :].todense())
        edits = np.array(ddda_data.edit_dict[locus][selected_read_inds, :].todense())
        
        track_ac = np.mean(edits, axis=0)
        track_ac_smoothed = np.convolve(track_ac, np.ones(50), mode='same') / 50
        
        plot_range = np.arange(500, len(track_ac_smoothed) - 500)
        
        fig = plt.figure(figsize=(8, 6), dpi=200)
        gs = gridspec.GridSpec(4, 1, height_ratios=[0.5, 2, 2, 2])
        
        ax0 = plt.subplot(gs[0])
        x_values = np.arange(len(track_ac_smoothed[plot_range]))
        ax0.fill_between(x_values, track_ac_smoothed[plot_range], color='purple', alpha=1)
        ax0.set_xlim([x_values.min(), x_values.max()])
        ax0.set_ylim([0, 0.1])
        ax0.set_title(f'{sample_name} TDAC-seq')
        ax0.set_ylabel('DddA\nMutation\nRate', fontsize=8)
        
        ax1 = plt.subplot(gs[1])
        read_edit_num = np.sum(CtoT_edits, axis=1)
        row_order = [i for i in np.argsort(-read_edit_num)]
        ax1.imshow(CtoT_edits[row_order, :][:, plot_range], aspect='auto', vmax=0.7, vmin=0, cmap=custom_cmap_blue)
        ax1.set_ylabel('CtoT edits')
        
        ax2 = plt.subplot(gs[2])
        read_edit_num = np.sum(GtoA_edits, axis=1)
        row_order = [i for i in np.argsort(-read_edit_num)]
        ax2.imshow(GtoA_edits[row_order, :][:, plot_range], aspect='auto', vmax=0.7, vmin=0, cmap=custom_cmap_red)
        ax2.set_ylabel('GtoA edits')
        
        ax3 = plt.subplot(gs[3])
        read_edit_num = np.sum(edits, axis=1)
        row_order = [i for i in np.argsort(-read_edit_num)]
        ax3.imshow(edits[row_order, :][:, plot_range], aspect='auto', vmax=0.5, vmin=0, cmap=custom_cmap_purple)
        ax3.set_xlabel('Region')
        ax3.set_ylabel('All edits')
        
        for ax in [ax0, ax1, ax2, ax3]:
             ax.spines['top'].set_visible(False)
             ax.spines['right'].set_visible(False)
             ax.set_xticks([])
             if ax != ax0: ax.set_yticks([])
        
        plt.tight_layout()
        plt.subplots_adjust(hspace=0.05)
        
        fig_path = f'{figure_dir}/{sample_name}_tdac_analysis.pdf'
        plt.savefig(fig_path, format='pdf', bbox_inches='tight')
        plt.close()
        logger.info(f"Saved: {fig_path}")
        
        logger.info(f"\n{'='*60}\nâœ… Sample {sample_name} complete\n{'='*60}\n")
        
        return {
            'sample_name': sample_name,
            'accessibility': track_ac_smoothed,
            'ddda_data': ddda_data,
            'status': 'success'
        }
    
    except Exception as e:
        logger.error(f"ERROR processing {sample_name}: {str(e)}", exc_info=True)
        return {
            'sample_name': sample_name,
            'accessibility': None,
            'ddda_data': None,
            'status': 'failed',
            'error': str(e)
        }



# ==============================================================================
# === MAIN PIPELINE ============================================================
# ==============================================================================



if __name__ == "__main__":
    
    parser = argparse.ArgumentParser(description="Run TDAC-seq pipeline (parallelized)")
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('--pod5', type=str, help="Path to pod5 folder")
    group.add_argument('--bam', type=str, help="Path to basecalled BAM")
    group.add_argument('--pod5-archive', type=str, help="Path to pod5 .tar.gz archive")
    
    args = parser.parse_args()
    
    if args.pod5:
        start_type = "pod5"
        input_path = os.path.abspath(args.pod5)
    elif args.bam:
        start_type = "bam"
        input_path = os.path.abspath(args.bam)
    elif args.pod5_archive:
        start_type = "archive"
        input_path = os.path.abspath(args.pod5_archive)
    
    # Run preprocessing
    run_preprocessing_shell(base_dir, RUN_TDAC_SH, genome_file, start_type, input_path)


    # Load samples
    samples = load_sample_config(base_dir)
    sample_list = list(samples.items())
    
    N_CORES_TOTAL = multiprocessing.cpu_count()
    # We will pass a core budget to the consensus step. Let's reserve 2 cores.
    N_CORES_FOR_CONSENSUS = max(1, N_CORES_TOTAL - 2)
    
    print(f"\n{'='*60}")
    print(f"ðŸš€ SERIAL SAMPLE PROCESSING")
    print(f"{'='*60}")
    print(f"Total Cores: {N_CORES_TOTAL}")
    print(f"Samples to process: {len(sample_list)}")
    print(f"SPOA Consensus Cores: {N_CORES_FOR_CONSENSUS}")
    print(f"(Alignment step will use its own pool)")
    print(f"{'='*60}\n")
    
    # Create partial function with fixed arguments
    worker_func = partial(
        process_single_sample,
        base_dir=base_dir,
        ref_range_dict=ref_range_dict,
        genome_file=genome_file,
        bias_model=bias_model,
        locus=locus,
        figure_dir=figure_dir,
        data_dir=data_dir,
        run_prefix=RUN_PREFIX,
        n_cores_for_consensus=N_CORES_FOR_CONSENSUS
    )
    
    print(f"[*] Starting serial processing...")
    results = []
    for sample_data in sample_list:
        # Call the worker function directly
        results.append(worker_func(sample_data))
    
    # Aggregate results
    print(f"\n{'='*60}")
    print(f"Aggregating results...")
    print(f"{'='*60}\n")
    
    all_accessibility = {}
    processed_data = {}
    failed_samples = []
    
    for result in results:
        sample_name = result['sample_name']
        if result['status'] == 'success':
            all_accessibility[sample_name] = result['accessibility']
            processed_data[sample_name] = result['ddda_data']
            print(f"âœ“ {sample_name}")
        else:
            failed_samples.append(sample_name)
            print(f"âœ— {sample_name}: {result.get('error', 'Unknown error')}")
    
    print(f"\nProcessed: {len(processed_data)} samples")
    if failed_samples:
        print(f"Failed: {len(failed_samples)} samples - {failed_samples}")
    
    # Combined plot
    if not all_accessibility:
        print("\n[ERROR] No successful samples processed. Exiting.")
        sys.exit(1)
    
    print(f"\n{'='*60}\nGenerating combined plot\n{'='*60}")


    plot_range = np.arange(500, len(list(all_accessibility.values())[0]) - 500)
    all_values = [v for track in all_accessibility.values() for v in track[plot_range]]
    y_min, y_max = 0, max(all_values) * 1.1


    n_samples = len(processed_data)
    fig_height = 2 + (n_samples * 2)


    fig = plt.figure(figsize=(12, fig_height), dpi=200)
    gs = gridspec.GridSpec(n_samples + 1, 1, height_ratios=[1] + [2] * n_samples)


    ax0 = plt.subplot(gs[0])
    color_palette = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']
    sample_colors = {}
    for idx, sample in enumerate(sorted(all_accessibility.keys())):
        sample_colors[sample] = color_palette[idx % len(color_palette)]


    for sample, track in all_accessibility.items():
        ax0.plot(plot_range, track[plot_range], 
                 label=sample, color=sample_colors[sample],
                 linewidth=1.5, alpha=0.8)


    ax0.set_ylabel('DddA\nAccessibility', fontsize=10)
    ax0.set_ylim([y_min, y_max]) 
    ax0.set_xlim([plot_range.min(), plot_range.max()])
    ax0.set_title('Combined TDAC-seq Analysis', fontsize=12)
    ax0.legend(loc='upper right', fontsize=9)
    ax0.spines['top'].set_visible(False)
    ax0.spines['right'].set_visible(False)
    ax0.set_xticks([])


    for idx, sample in enumerate(sorted(processed_data.keys())):
        ddda_data = processed_data[sample]
        min_num = 8000 
        
        all_deduped_inds = ddda_data.deduped_inds[locus]
        total_deduped_reads = len(all_deduped_inds)
        
        if total_deduped_reads >= min_num:
            undel_read_inds = np.random.choice(all_deduped_inds, min_num, replace=False)
        else:
            undel_read_inds = all_deduped_inds
        
        edits = np.array(ddda_data.edit_dict[locus][undel_read_inds, :].todense())
        read_edit_num = np.sum(edits, axis=1)
        row_order = [i for i in np.argsort(-read_edit_num)]
        
        ax = plt.subplot(gs[idx + 1])
        ax.imshow(edits[row_order, :][:, plot_range], 
                  aspect='auto', vmax=0.5, vmin=0, cmap=custom_cmap_purple)
        ax.set_ylabel(f'{sample}\nEdits', fontsize=9)
        ax.set_yticks([])
        
        if idx < n_samples - 1:
            ax.set_xticks([])
        else:
            ax.set_xlabel('Genomic position', fontsize=10)
            ax.set_xticks([])


    plt.tight_layout()
    plt.subplots_adjust(hspace=0.05)


    combined_path = f'{figure_dir}/combined_analysis.pdf'
    plt.savefig(combined_path, format='pdf', bbox_inches='tight')
    plt.close()
    print(f"âœ“ Saved: {combined_path}")
    
    print(f"\n{'='*60}\nâœ… PIPELINE COMPLETE\n{'='*60}")
    print(f"âœ“ Processed {len(processed_data)} samples")
    print(f"âœ“ Figures saved to: {figure_dir}")
    print(f"âœ“ Data saved to: {data_dir}")
    print(f"{'='*60}\n")
